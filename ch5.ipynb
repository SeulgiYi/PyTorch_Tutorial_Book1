{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "26\n",
      "torch.Size([104, 13])\n",
      "torch.Size([104])\n",
      "(tensor([1.3710e+01, 1.8600e+00, 2.3600e+00, 1.6600e+01, 1.0100e+02, 2.6100e+00,\n",
      "        2.8800e+00, 2.7000e-01, 1.6900e+00, 3.8000e+00, 1.1100e+00, 4.0000e+00,\n",
      "        1.0350e+03]), tensor(0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seulgi/anaconda3/envs/ptlesson/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 4.813877642154694\n",
      "100 4.798457860946655\n",
      "150 4.781480491161346\n",
      "200 4.814006567001343\n",
      "250 4.815882444381714\n",
      "300 4.782549977302551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##(5.2)\n",
    "# PyTorch Library\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# scikit-learn Library\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pandas Library\n",
    "import pandas as pd\n",
    "\n",
    "##(5.3) load wine data\n",
    "wine = load_wine() \n",
    "\n",
    "##(5.4) print data using DataFrame\n",
    "pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "##(5.5) print target \n",
    "wine.target\n",
    "\n",
    "##(5.6) assign 130 of data and target into variable\n",
    "wine_data = wine.data[0:130]\n",
    "wine_target = wine.target[0:130]\n",
    "\n",
    "##(5.7) divide dataset: 80% of training set and 20% of test set\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(wine_data, wine_target, test_size=0.2)\n",
    "print(len(train_X))\n",
    "print(len(test_X))\n",
    "\n",
    "##(5.8) convert numpy to torch tensor\n",
    "train_X = torch.from_numpy(train_X).float()\n",
    "train_Y = torch.from_numpy(train_Y).long()\n",
    "test_X = torch.from_numpy(test_X).float()\n",
    "test_Y = torch.from_numpy(test_Y).long()\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "\n",
    "##(5.9) merge data and target into one tensor\n",
    "### TensorDataset: Dataset wrapping tensors that have the same size of first dimension.\n",
    "train = TensorDataset(train_X, train_Y) \n",
    "print(train[0])\n",
    "### DataLoader: set minibatches which are size of 16 and randomly shuffled\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "\n",
    "##(5.10) Establish Neural Network \n",
    "### class torch.nn.Module: Base class for all NN modules\n",
    "### Net class inherits nn.Module(parent class)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()   ## utilize __init__ method of parent class\n",
    "        self.fc1 = nn.Linear(13,96)   ## torch.nn.Linear(in,out,bias=True) \n",
    "        self.fc2 = nn.Linear(96, 2)   ##  : applies linear transformation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))       ## Hidden layer: ReLU \n",
    "        x = self.fc2(x)               \n",
    "        return F.log_softmax(x)       ## Output layer: softmax\n",
    "    \n",
    "### create instance\n",
    "model = Net()\n",
    "\n",
    "##(5.11)\n",
    "### loss function: cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### optimizer: stochastic gradient descent, learning rate 1%\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "### start learning \n",
    "for epoch in range(300):\n",
    "    total_loss = 0\n",
    "    ### pop minibatche set out from train_loader\n",
    "    for train_x, train_y in train_loader:\n",
    "        ### graph \n",
    "        train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "        ### initialize gradient as PyTorch accumulates gradients\n",
    "        optimizer.zero_grad()\n",
    "        ### calc. forward propagation\n",
    "        output = model(train_x)\n",
    "        ### calc. loss function\n",
    "        loss = criterion(output, train_y)\n",
    "        ### calc. backward propagation\n",
    "        loss.backward()\n",
    "        ### update weights\n",
    "        optimizer.step()\n",
    "        ### calc. cumulative loss func\n",
    "        total_loss += loss.data.item()\n",
    "    if (epoch+1)%50 == 0:\n",
    "        print(epoch+1, total_loss)\n",
    "        \n",
    "        \n",
    "##(5.12) \n",
    "### graph\n",
    "### torch.autograd.Variable(data): wrapping tensor and logging \n",
    "test_x, test_y = Variable(test_X), Variable(test_Y)\n",
    "### make output 0 or 1\n",
    "result = torch.max(model(test_x).data, 1)[1]\n",
    "### accuracy of model\n",
    "accuracy = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\n",
    "\n",
    "accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptlesson] *",
   "language": "python",
   "name": "conda-env-ptlesson-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
